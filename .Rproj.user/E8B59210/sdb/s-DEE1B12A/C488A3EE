{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Problem4\"\nauthor: \"Lawrence Chan\"\ndate: \"12/1/2017\"\noutput: pdf_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n## Part a\n\nWe begin by loading the dataset:\n\n```{r}\nfertility <- read.csv(\"~/Downloads/fertility1.txt\")\nattach(fertility)\nhead(fertility)\n```\n\nNext, we regress kids against the explanatory variables:\n```{r}\ntimeasfactor <- as.factor(time)\nmodel1 <- lm(kids ~ educ + age +  I(age^2) + region + black + timeasfactor\n             + age * educ + region * educ + black * educ)\nsummary(model1)\nanova(model1)\n```\nBased on the results of this regression, it seems that education decreases fertility (term for educ is very statistically significant), education for African-Americans further decreases fertility (the interaction term educ:black is statistically significant), and education increases fertility for older women compared to younger women (the interaction term educ:age is also highly statistically significant). Finally, education seems reduce fertility more for women in the Northcentral region (the interaction term educ:regionnorthcentral is statistically significant), at least compared for women in the East. \n\nReading the parameters off the table, slope of education is given by:\n$$-0.603 + 0.012 age - 0.118 northcentral + 0.002 south - 0.098 west - 0.122 black.$$\nNote that the coefficients for south and west are not statistically significant.\n\nThe fact that education decreases fertility is unsuprising, since it seems plausible that women willing to spend more time in school both have less time to have children and less immediate desire for children. The coefficient for black is both highly statistical significant and large, and it seems plausible that education has a similar effect or predictive value for childrearing tendencies for both African-American and white women, but the fact that African American women would have had more children at low levels of education means that the effect of education may appear more pronounced. The fact that the effect of education on fertility is smaller for older women suggests that women pursuing more education may simply be delaying having children instead of not having them. The story of the interaction term between education and being in the North Central region is likely similar to the interaction between education and race; since women in the North Central region are more likely to have kids at lower levels of education than women in the East region, the effect of education appears more pronounced. \n\n## Part b \n\n```{r part b}\ntime <- as.numeric(time)\nmodel2 <- lm(kids ~ educ + age +  I(age^2) + region + black +\n               time + age * educ + region * educ + black * educ)\nsummary(model2)\n```\n\nTime is highly statistically significant, with a one-point increase in time prediction 0.118 fewer children. Since there are 7 data points for time, we consider regressing up to the sixth power of time (since a sixth-order polynomial can exactly fit 7 data points):\n```{r part b 2}\nmodel3 <- lm(kids ~ educ + age +  I(age^2) + region + black \n             + time + I(time^2) + I(time^3) + I(time^4) + I(time^5) + I(time^6)\n             + age * educ + region * educ + black * educ)\nsummary(model3)\n```\nNone of these six factors are statistically significant. We repeat this process by including only up to the fith powers of time in the regression:\n``` {r part b 3}\nmodel4 <- lm(kids ~ educ + age +  I(age^2) + region + black \n             + time + I(time^2) + I(time^3) + I(time^4) + I(time^5)\n             + age * educ + region * educ + black * educ)\nsummary(model4)\n```\nNow all the coefficients in front of powers of time are significant! So we need 5 powers to capture the variation attributable to time. In part a., we had 6 coefficients associated with time. This approach now has 5 coefficients. So we save one degree of freedom in total. \n\n## Part c.\n\nThis is fairly rote:\n``` {r part c}\nqqnorm(resid(model1))\nqqline(resid(model1))\n```\nIn general, the residuals seem fairly on close to being normal. However, compared to normality, some of the left tail of the residuals seem little too short, and the residuals on the right side seem a little too large. \n\n## Part d\n\nWe first test for heteroscedasticity graphically by looking at the residuals of the model:\n```{r part d 1}\nplot(model1)\n```\nFrom the Residuals vs Fitted plot, there doesn't seem to be much heteroscedasticity. However, the Scale-Location plot, we see clear evidence of heteroscedasticity. (In addition to some increasing pattern which is likely due to the fact that the number of kids takes on only integral values.)\n\nTo test for heteroscedasticity statistically, we run the Breusch-Pagan test. We start by regressing the squared residuals against the explanatory variables:\n```{r part d 2}\nresid2 <- resid(model1)^2\nresid_model <- lm(resid2 ~ educ + age +  I(age^2) + region + black + timeasfactor + age * educ + region * educ + black * educ)\nsummary(resid_model)\n```\nThe test statistic for this test is the Lagrange multiplier:\n$$ \\textrm{LM} = nR^2, $$\nthat is:\n``` {r part d 3}\nnR2 <- length(year) * summary(resid_model)$r.squared\n```\nThe test statistic is asymptotically chi-squared distributed with 18 degrees of freedom. So we can calculate a $p$-value as follows:\n```{r part d 4}\n1 - pchisq(nR2, 18)\n```\nSince the p-value is tiny, we can strongly reject the null hypothesis of homoscedasticity. (Alternatively, note the p-value of the F statistic for the residual is also very small, which allows us to reject the null hypothesis of homoscedasticity in a different manner.) This suggests that the homscedastic standard errors (and the p-values for the regression in part a) may be too small. \n\nWe can confirm this using the sandwich package.\n```{r part d 5}\nlibrary(\"sandwich\")\nsqrt(diag(vcov(model1)))\nsqrt(diag(vcovHC(model1, type=\"HC1\")))\nsqrt(diag(vcovHC(model1, type=\"HC2\")))\nsqrt(diag(vcovHC(model1, type=\"HC3\")))\nsqrt(diag(vcovHC(model1, type=\"HC4\")))\n```\nNote that the heteroscedasticity-robust standard errors are in general quite a bit larger than the homoscedastic standard errors.\n## Part e\n```{r part e}\nmodel5 <- lm(kids ~ educ + age +  I(age^2) + region + black + timeasfactor\n             + age * educ + region * educ + black * educ + time*educ)\nsummary(model5)\n```\nThe coefficient for educ\\*time is not statistically significant. This suggests that the effect of education on fertility does not vary with the year during the time period studied. Since the heteroscedasiticity-robust standard errors are larger than the homoscedasitic standard errors, this result does not change if we employ heteroscedasiticity-robust standard errors for the p-value calculation. \n\n## Part f \n\nWe start by fitting first-stage regressions for each of the variables involving education: educ, educ:age, educ:northcentral, educ:south, educ:west, and educ:black. We do each in turn, checking for significant partial correlation with explanatory variables. \n\nEduc:\n```{r part f}\neduc_model <- lm (educ ~ meduc +  feduc + age +  I(age^2) + region \n                + black + timeasfactor\n                + age * meduc + region * meduc + black * meduc \n                + age * feduc + region * feduc + black * feduc )\nsummary(educ_model)\neduc_pred <- predict(educ_model)\n```\nInterestingly, while the coefficient for meduc term is significant (assuming homoscedasiticity), the coefficient for the feduc term is not. The significance of the terms for meduc, meduc:west, feduc:south, and feduc:west suggests that the instruments are significantly partially correlated with the variable of interest (recall the results from class and problem 2 of this problem set).\n\nEduc:age:\n```{r part f 2}\neduc.age_model <- lm (educ*age ~ meduc +  feduc + age +  I(age^2) + region \n                + black + timeasfactor\n                + age * meduc + region * meduc + black * meduc \n                + age * feduc + region * feduc + black * feduc )\nsummary(educ.age_model)\neduc.age_pred <- predict(educ.age_model)\n```\nNote that neither meduc nor feduc predict educ:age, as we might expect.However, the significance of coefficients of the terms for meduc:west, feduc:south, and feduc:west suggests that the instruments are significantly partially correlated with the variable of interest.\n\nEduc:northcentral:\n```{r part f 3}\neduc.central_model <- lm (educ*northcentral ~ meduc +  feduc + age +  I(age^2)\n                          + region + black + timeasfactor\n                          + age * meduc + region * meduc + black * meduc \n                          + age * feduc + region * feduc + black * feduc )\nsummary(educ.central_model)\neduc.northcentral_pred <- predict(educ.central_model)\n```\nInterestingly, only meduc:northcentral and feduc:northcentral predict educ:northcentral. Since these two terms have significant coefficients, the instruments are significantly partially correlated with the variable of interest. \n\nEduc:south:\n```{r part f 4}\nsouth = as.numeric(region==\"south\")\neduc.south_model <- lm (educ*south ~ meduc +  feduc + age +  I(age^2)\n                          + region + black + timeasfactor\n                          + age * meduc + region * meduc + black * meduc \n                          + age * feduc + region * feduc + black * feduc )\nsummary(educ.south_model)\neduc.south_pred <- predict(educ.south_model)\n```\nInterestingly, of our instruments, only meduc:south and feduc:south predict educ:south. However, since these two terms have significant coefficients, the instruments are significantly partially correlated with the variable of interest. \n\nEduc:west:\n```{r part f 5}\neduc.west_model <- lm (educ*west ~ meduc +  feduc + age +  I(age^2)\n                          + region + black + timeasfactor\n                          + age * meduc + region * meduc + black * meduc \n                          + age * feduc + region * feduc + black * feduc )\nsummary(educ.west_model)\neduc.west_pred <- predict(educ.west_model)\n```\nAs with northcentral and south, only meduc:west and feduc:west predict educ:west. However, since these two terms have significant coefficients, the instruments are significantly partially correlated with the variable of interest. \n\nEduc:black\n```{r part f 6}\neduc.black_model <- lm (educ*black ~ meduc +  feduc + age +  I(age^2)\n                          + region + black + timeasfactor\n                          + age * meduc + region * meduc + black * meduc \n                          + age * feduc + region * feduc + black * feduc )\nsummary(educ.black_model)\neduc.black_pred <- predict(educ.black_model)\n```\nAs with the region interaction terms, only meduc:black and feduc:black predict educ:west. However, since these two terms have significant coefficients, the instruments are significantly partially correlated with the variable of interest. \n\nNext, we can compute our 2SLS. \n```{r part f 7}\nmodel6 <- lm(kids ~ educ_pred + region + age + I(age^2) + black\n             + timeasfactor + educ.age_pred\n             + educ.west_pred + educ.northcentral_pred + educ.south_pred \n             + educ.black_pred)\nsummary(model6)\n```\nNote that for 2SLS, the actual standard error of the estimate is given by (15) in notes 10. So we need to apply the appropriate correction for the standard errors here. \n```{r part f 8}\nbeta_iv <- coef(model6)\nsum_tilde_resid <- sum(resid(model6) * resid(model6))\nx <- data.frame(rep(0,1129), educ, region, black, age, age^2, time, age*educ,\n                west*educ, northcentral*educ, south*educ, black*educ)\nhat_resid <- kids - predict(model1, data= x)\nratio = sum(hat_resid * hat_resid)/sum_tilde_resid\ncorrected_vcov <- matrix(ratio %o% vcov(model6),ncol=19)\nsqrt(diag(corrected_vcov))\n```\nThis is assuming homoscedasiticty, of course. To compute the heteroscedasiticty-robust standard errors, we invoke the sandwich package again:\n``` {r part f 9}\nsqrt(diag(vcovHC(model6,type=\"HC1\")))\nsqrt(diag(vcovHC(model6,type=\"HC2\")))\nsqrt(diag(vcovHC(model6,type=\"HC3\")))\nsqrt(diag(vcovHC(model6,type=\"HC4\")))\n```\nWe can do a comparision between the parameter estimates and their standard errors using these results:\n```{r part f 10}\nols.coef <- coef(model1)\nols.seho <- sqrt(diag(vcov(model1)))\nols.sehe <- sqrt(diag(vcovHC(model1, type=\"HC1\")))\ntsls.coef <- coef(model6)\ntsls.seho <- sqrt(diag(vcov(model6)))\ntsls.sehe <- sqrt(diag(vcovHC(model6,type=\"HC1\")))\nd <- data.frame(ols.coef, ols.seho, ols.sehe)\nd\n#now show the 2sls results:\nd2 <- data.frame(tsls.coef, tsls.seho, tsls.sehe)\nd2\n```\nInterestingly, the coefficient for black is significantly larger for the 2sls regression than the ols regression - 3.92 instead of 2.66! However, the black:educ interaction is also stronger for 2sls - $-0.222$ instead $-0.122$ - which helps mitigate this effect. For example, for 12.69 years of education (the average in the dataset), the difference of the racial effect is only $3.92 - 0.222*12.69 = 1.10$ versus $2.66 - 0.122*12.69 = 1.11$, basically the same. Other than the racial flag and its interaction term with education, most of the other coefficients are very similar. \n\n## Part g\nWe perform the Haussman test, regressing kids against each of the explanatory variables and the vector of first-stage residuals. \n```{r part g}\nmodel8<-lm(kids~educ+age+I(age^2)+region+black+timeasfactor+educ*region+resid(educ_model)+resid(educ.age_model)+resid(educ.south_model)+resid(educ.central_model)+resid(educ.west_model)+resid(educ.black_model))\nsummary(model8)\n\n```\nNext, we compute the test statistic and p-value, assuming homoscedasticity:\n\n```{r part g 2} \nvcov_H <-vcov(model8)[15:20,15:20]\nbeta_H <-coef(model8)[15:20]\nF_stat <-t(beta_H)%*%solve(vcov_H)%*%beta_H/6\nF_stat\n1-pchisq(F_stat,6) # p-value, since this statistic is chi-sq distributed under null\n```\nThis is *very* not significant. We can do the same thing using heteroscedasticity-robust estimators:\n```{r part g 3} \nvcov_HC <-vcovHC(model8, type=\"HC1\")[15:20,15:20]\nbeta_H <-coef(model8)[15:20]\nF_HC <-t(beta_H)%*%solve(vcov_HC)%*%beta_H/6\nF_HC\n1-pchisq(F_HC,6) #p-value\n```\nThe p-value gets even larger, as the heteroscedasticity-robust standard errors are larger. In either case, there does not seem to be evidence of endogeneity. (This is consistent with the fact that the OLS and 2SLS estimates of coefficients were very similar in the previous section.)\n## Part h\nWe follow the guideline set in note 11. We start by regressing the residuals from our 2SLS regression against the exogenous variables and instruments:\n``` {r part h}\nmodel9 <- lm(hat_resid ~ age + I(age^2) + region + \n    black + timeasfactor + meduc + feduc + meduc * age + feduc * age + \n    meduc * region + feduc*region + meduc * black + feduc * black)\nsummary(model9)\n```\nNone of the coefficients are significant. Indeed, the Lagrange Multiplier statistic $NR_u^2$ has value:\n```{r}\nnR_u2<- length(year) * (summary(model9)$r.squared)\nnR_u2\n```\nAnd $p$-value\n```{r}\n1-pchisq(nR_u2, 6) \n```\nAnd so we do not reject the null hypothesis, and it seems that there is no evidence of overidentification. ",
    "created" : 1513137015811.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3401018689",
    "id" : "C488A3EE",
    "lastKnownWriteTime" : 1512163157,
    "last_content_update" : 1512163157,
    "path" : "~/HW4Problem4.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}